
name: 'ManiGaussian_BC'

use_fabric: True

use_depth: True
use_neural_rendering: True
num_view_for_nerf: 20

# choices: T5 (768), CLIP (512)
# we use CLIP as language model. Users could explore more about other language models such as T5.
language_model: 'CLIP'
language_model_dim: 512

# Voxelization
image_crop_size: 64
bounds_offset: [0.15]
voxel_sizes: [100]
include_prev_layer: False

# Perceiver
num_latents: 2048
latent_dim: 512
transformer_depth: 6  # default: 6
transformer_iterations: 1
cross_heads: 1
cross_dim_head: 64
latent_heads: 8
latent_dim_head: 64
pos_encoding_with_lang: True
conv_downsample: True
lang_fusion_type: 'seq' # or 'concat'
voxel_patch_size: 5
voxel_patch_stride: 5
final_dim: 128

# Training
input_dropout: 0.1
attn_dropout: 0.1
decoder_dropout: 0.0

lr: 0.0005  # GNFactor
lr_scheduler: False
num_warmup_steps: 3000
optimizer: 'lamb' # or 'adam'
lambda_weight_l2: 0.000001  # weight_decay

# BC loss coeffs
trans_loss_weight: 1.0
rot_loss_weight: 1.0
grip_loss_weight: 1.0
collision_loss_weight: 1.0
rotation_resolution: 5

# Network
activation: lrelu
norm: None

# Augmentation
crop_augmentation: True
transform_augmentation:
  apply_se3: True
  aug_xyz: [0.125, 0.125, 0.125]
  aug_rpy: [0.0, 0.0, 45.0]
  aug_rot_resolution: ${method.rotation_resolution}

demo_augmentation: True
demo_augmentation_every_n: 10

# Ablations
no_skip_connection: False
no_perceiver: False
no_language: False
keypoint_method: 'heuristic'

# wandb logger
use_wandb: True # covered in bash file

lambda_bc: 1.0

# Auxiliary Loss
neural_renderer:
  # Gaussian Splatting (GS)
  visdom: False
  render_freq: 1000

  use_clip: False

  use_dynamic_field: False  # please set use_neural_rendering true first

  # weight for all auxiliary losses
  lambda_nerf: 0.01

  # ManiGaussian loss weight. 
  lambda_embed: 0.01 # Semantic Feature Consistency Loss.
  lambda_rgb: 1.0 # Current Scene Consistency Loss.
  lambda_l1: 1.0 # L1 loss for Gaussian Splatting
  lambda_ssim: 0.0 # SSIM loss for Gaussian Splatting

  lambda_dyna: 0.01
  lambda_reg: 0.0 # not used

  dataset:
    use_processed_data: True
    bg_color: [0, 0, 0]
    # bg_color: [0, 0.196, 0.220] # teal (deep green) color for RLBench background
    zfar: 4.0
    znear: 0.1
    trans: [0.0, 0.0, 0.0]
    scale: 1.0
    mask_gt_rgb: False

  foundation_model_name: null # or 'diffusion' or 'dinov2'
  d_embed: 3
  loss_embed_fn: cosine
  
  d_latent: 128
  
  d_lang: 128
  
  voxel_shape: 100
  image_width: 128
  image_height: 128
  coordinate_bounds: [-0.3, -0.5, 0.6, 0.7, 0.5, 1.6]

  use_code: True
  use_code_viewdirs: False
  use_xyz: True

  mlp:
    n_blocks: 5
    d_hidden: 512
    combine_layer: 3
    combine_type: average
    beta: 0.0
    use_spade: False

    opacity_scale: 1.0
    opacity_bias: -2.0
    scale_bias: 0.02
    scale_scale: 0.003
    xyz_scale: 0.1
    xyz_bias: 0.0
    max_sh_degree: 1

  next_mlp:
    d_in: 3
    d_lang: 128
    d_out: 3
    n_blocks: 5
    d_hidden: 512
    combine_layer: 3
    combine_type: average
    beta: 0.0
    use_spade: False
    warm_up: 3000
    use_action: True

  code:
    num_freqs: 6
    freq_factor: 1.5
    include_input: True







